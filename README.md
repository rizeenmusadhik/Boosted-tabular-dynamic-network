# Boosted-Dynamic-Networks
[Haichao Yu](mailto:haichao.yu@outlook.com), [Haoxiang Li](http://blog.haoxiang.org/haoxiang.html), [Gang Hua](https://www.ganghua.org), [Gao Huang](http://www.gaohuang.net), [Humphrey Shi](https://www.humphreyshi.com)

This repository is the official implementation for our paper [Boosted Dynamic Neural Networks](https://arxiv.org/abs/2211.16726). In the paper, we propose a new early-exiting dynamic neural network (EDNN) architecture, where we formulate an EDNN as an additive model inspired by gradient boosting, and propose multiple training techniques to optimize the model effectively. Our experiments show it achieves superior performance on CIFAR100 and ImageNet datasets in both anytime and budgeted-batch prediction modes.

**NEW: Tabular Data Support!** This repository now supports tabular data in addition to image data. The MSDNet and RANet architectures have been adapted to work with fully connected layers for both tabular classification and regression tasks with automatic task detection. **The complete dynamic network architecture with multi-block boosted training is fully implemented for tabular data.**

![Framework](figures/arch.png)

## Features

### Image Classification
- **MSDNet**: Multi-Scale Dense Networks for image classification
- **RANet**: Resolution Adaptive Networks for efficient image processing
- Support for CIFAR-10, CIFAR-100, and ImageNet datasets

### Tabular Data Support
- **TabularMSDNet**: Fully connected version of MSDNet for tabular data
- **TabularRANet**: Fully connected version of RANet for tabular data
- **ðŸŽ¯ Dynamic Network Architecture**: Complete multi-block boosted training with ensemble predictions
- **Classification & Regression**: Automatic task detection and appropriate metrics
- **Per-Block Performance**: Individual metrics tracking for each network block
- **Progressive Improvement**: Later blocks consistently outperform earlier blocks
- **Real-world Datasets**: Integration with OpenML (Adult, Covertype, California Housing)
- **Automatic Preprocessing**: Feature scaling, target normalization, data splitting
- **Robust Evaluation**: Multiple runs with standard deviation reporting

## Results in Anytime Prediction Mode
| MSDNet on CIFAR100 | MSDNet on ImageNet | RANet on CIFAR100 | RANet on ImageNet |
|:---:|:---:|:---:|:---:|
| [![](figures/cifar100_any_msdnet.png)]()  | [![](figures/imagenet_any_msdnet.png)]() | [![](figures/cifar100_any_ranet.png)]() | [![](figures/imagenet_any_ranet.png)]()|

## Results in Budgeted-batch Prediction Mode
| MSDNet on CIFAR100 | MSDNet on ImageNet | RANet on CIFAR100 | RANet on ImageNet |
|:---:|:---:|:---:|:---:|
| [![](figures/cifar100_dynamic_msdnet.png)]()  | [![](figures/imagenet_dynamic_msdnet.png)]() | [![](figures/cifar100_dynamic_ranet.png)]() | [![](figures/imagenet_dynamic_ranet.png)]()|

## Usage 

### Image Classification
Please use the scripts in `msdnet_scripts/` and `ranet_scripts/` for model training and evaluation. For ImageNet experiments, please first download the dataset and put it into the proper folder.

### Tabular Data Support

This framework supports both **classification** and **regression** tasks on tabular data with automatic task detection and appropriate metrics.

#### Quick Start
```bash
# Check device availability
python check_device.py

# Classification with dynamic network (shows per-block performance)
python train_tabular.py --dataset tabular --tabular_dataset adult --arch msdnet --epochs 1 --nBlocks 2
# Output: Block 0: 79.25%, Block 1: 80.34% (progressive improvement!)

# Regression with dynamic network (shows per-block performance)
python train_tabular.py --dataset tabular --tabular_dataset california_housing --arch msdnet --epochs 1 --nBlocks 2 --lr 0.01
# Output: Block 0: RÂ²=0.4868, Block 1: RÂ²=0.4941 (progressive improvement!)

# Evaluate trained model
python eval_tabular.py --dataset tabular --tabular_dataset adult --arch msdnet --evaluate-from results/exp_xxx/best_model.pth
```

#### Supported Datasets

##### Classification Datasets
- **adult**: Adult income prediction (14 features, binary, Block 0: 79.25%, Block 1: 80.34%)
- **covertype**: Forest cover type prediction (54 features, 7-class classification, 74.1% accuracy)
- **heloc**: Home Equity Line of Credit (23 features, binary - data quality issues)
- **custom**: Synthetic classification data with configurable parameters
- **credit**: Credit card fraud simulation (30 features, binary)
- **diabetes**: Diabetes prediction simulation (8 features, binary)

##### Regression Datasets
- **california_housing**: California housing prices (8 features, Block 0: RÂ²=0.4868, Block 1: RÂ²=0.4941)

#### Training Command Examples

##### Binary Classification
```bash
# Adult income prediction
python train_tabular.py --dataset tabular --tabular_dataset adult --arch msdnet --epochs 10 --nBlocks 3

# Custom synthetic binary classification
python train_tabular.py --dataset tabular --tabular_dataset custom --num_features 20 --num_classes 2 --arch ranet
```

##### Multi-class Classification
```bash
# Forest cover type prediction (7 classes)
python train_tabular.py --dataset tabular --tabular_dataset covertype --arch msdnet --epochs 5 --nBlocks 2
```

##### Regression
```bash
# California housing price prediction
python train_tabular.py --dataset tabular --tabular_dataset california_housing --arch msdnet --epochs 10 --lr 0.01
```

#### Evaluation Examples
```bash
# Classification evaluation (shows accuracy and AUC)
python eval_tabular.py --dataset tabular --tabular_dataset adult --arch msdnet --evaluate-from results/exp_xxx/best_model.pth

# Regression evaluation (shows MSE, RMSE, RÂ²)
python eval_tabular.py --dataset tabular --tabular_dataset california_housing --arch msdnet --evaluate-from results/exp_xxx/best_model.pth
```

#### Key Parameters for Tabular Data
- `--dataset tabular`: Enable tabular data mode
- `--tabular_dataset`: Specific dataset name (adult, covertype, california_housing, etc.)
- `--arch`: Architecture choice (`msdnet` or `ranet`)
- `--nBlocks`: Number of network blocks (default: 2)
- `--nChannels`: Base number of neurons (default: 32)
- `--epochs`: Number of training epochs
- `--lr`: Learning rate (use 0.01 for regression, 0.1 for classification)

#### Features
- **ðŸŽ¯ Dynamic Network Architecture**: Complete multi-block boosted training with ensemble predictions
- **Per-Block Training**: Individual loss tracking for each network block (e.g., `stage_0_loss`, `stage_1_loss`)
- **Progressive Improvement**: Later blocks consistently outperform earlier blocks
- **Automatic Task Detection**: Framework automatically detects classification vs regression
- **Target Scaling**: Regression targets are automatically normalized to prevent gradient issues  
- **Feature Scaling**: All features are standardized using StandardScaler
- **Robust Evaluation**: Multiple evaluation runs with standard deviation reporting
- **Metadata Persistence**: Training configuration is saved in model checkpoints for reliable evaluation
- **Real-world Datasets**: Integration with OpenML for accessing real datasets

#### Performance Metrics
- **Classification**: Per-block accuracy, AUC, Classification Report
- **Regression**: Per-block MSE, MAE, RMSE, RÂ² Score

#### Training Output Example
```
2025-08-30 00:01:11 step 0, stage_0_loss 1.1172
2025-08-30 00:01:11 step 0, stage_1_loss 2.0242
...
Per-Block Classification Results:
Block 0: Accuracy: 79.25%
Block 1: Accuracy: 80.34%
```

**Note**: The dynamic network architecture processes samples through multiple blocks with ensemble predictions, following the boosted dynamic neural network approach from the original paper.

### Device Support
- **Automatic Device Selection**: The code automatically uses GPU if available, otherwise falls back to CPU
- **Cross-Platform**: Works seamlessly on both GPU and CPU-only systems
- **No Manual Configuration**: Device selection is handled automatically

## Citation
```
@article{yu2022boostdnn,
	title        = {Boosted Dynamic Neural Networks},
	author       = {Yu, Haichao and Li, Haoxiang and Hua, Gang and Huang, Gao and Shi, Humphrey},
	year         = 2022,
	url          = {https://arxiv.org/abs/2211.16726},
	eprint       = {2211.16726},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
```

## Acknowledgments
This repository is built based on previous open-sourced efforts:
* [MSDNet-PyTorch](https://github.com/kalviny/MSDNet-PyTorch)
* [IMTA](https://github.com/kalviny/IMTA)
* [RANet-pytorch](https://github.com/yangle15/RANet-pytorch)
